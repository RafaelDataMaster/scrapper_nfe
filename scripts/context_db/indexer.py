"""
Indexador de documentos de contexto para ChromaDB.

CONCEITO:
=========
Este script l√™ todos os arquivos .md de docs/context/, divide cada um
em "chunks" (peda√ßos menores) e armazena no ChromaDB com seus embeddings.

POR QUE DIVIDIR EM CHUNKS?
==========================
1. Documentos grandes (ex: 5000 palavras) s√£o dif√≠ceis de buscar com precis√£o
2. Se voc√™ pergunta "como resolver timeout?", o documento inteiro √© muito gen√©rico
3. Dividindo em chunks de ~500 palavras, a busca retorna o trecho espec√≠fico
4. Cada chunk mant√©m metadados (arquivo de origem, posi√ß√£o) para refer√™ncia

ESTRUTURA DO CHROMADB:
======================
Collection "context_docs":
‚îú‚îÄ‚îÄ ID: hash √∫nico do chunk
‚îú‚îÄ‚îÄ Document: texto do chunk
‚îú‚îÄ‚îÄ Embedding: vetor de 384 dimens√µes
‚îî‚îÄ‚îÄ Metadata:
    ‚îú‚îÄ‚îÄ source: "docs/context/troubleshooting.md"
    ‚îú‚îÄ‚îÄ chunk_index: 0, 1, 2...
    ‚îî‚îÄ‚îÄ title: t√≠tulo extra√≠do do documento
"""

import sys
from pathlib import Path

# Adiciona raiz do projeto ao path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))

import chromadb
from chromadb.config import Settings
import hashlib
import re
from typing import List, Dict, Optional

from scripts.context_db.embeddings import EmbeddingManager


# Configura√ß√µes de chunking
CHUNK_SIZE = 500  # Palavras por chunk (aproximado)
CHUNK_OVERLAP = 50  # Palavras de sobreposi√ß√£o entre chunks


def extract_title(content: str) -> str:
    """
    Extrai o t√≠tulo do documento (primeiro # heading).

    Args:
        content: Conte√∫do markdown do arquivo

    Returns:
        T√≠tulo extra√≠do ou "Sem t√≠tulo"
    """
    # Procura por # no in√≠cio de linha (heading markdown)
    match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
    if match:
        return match.group(1).strip()
    return "Sem t√≠tulo"


def chunk_text(
    text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP
) -> List[str]:
    """
    Divide texto em chunks com sobreposi√ß√£o.

    COMO FUNCIONA:
    ==============
    Texto: "palavra1 palavra2 palavra3 palavra4 palavra5 palavra6"
    chunk_size=3, overlap=1

    Chunk 0: "palavra1 palavra2 palavra3"
    Chunk 1: "palavra3 palavra4 palavra5"  <- overlap: palavra3 repetida
    Chunk 2: "palavra5 palavra6"

    A sobreposi√ß√£o evita que informa√ß√£o seja "cortada" entre chunks.

    Args:
        text: Texto completo
        chunk_size: N√∫mero aproximado de palavras por chunk
        overlap: Palavras de sobreposi√ß√£o

    Returns:
        Lista de chunks de texto
    """
    # Divide por palavras (simplificado)
    words = text.split()

    if len(words) <= chunk_size:
        # Texto pequeno, retorna inteiro
        return [text]

    chunks = []
    start = 0

    while start < len(words):
        # Pega chunk_size palavras a partir de start
        end = start + chunk_size
        chunk_words = words[start:end]
        chunks.append(" ".join(chunk_words))

        # Avan√ßa com sobreposi√ß√£o
        # Se chunk_size=500 e overlap=50, avan√ßa 450 palavras
        start += chunk_size - overlap

    return chunks


def generate_chunk_id(source: str, chunk_index: int) -> str:
    """
    Gera ID √∫nico para um chunk.

    Usa hash MD5 do path + √≠ndice para garantir unicidade
    e permitir atualiza√ß√µes incrementais.
    """
    content = f"{source}::{chunk_index}"
    return hashlib.md5(content.encode()).hexdigest()


class ContextIndexer:
    """
    Indexa documentos de contexto no ChromaDB.

    Attributes:
        db_path: Caminho onde o ChromaDB persiste os dados
        collection: Collection do ChromaDB com os documentos
        embedding_manager: Gerenciador de embeddings
    """

    COLLECTION_NAME = "context_docs"

    def __init__(self, db_path: Optional[Path] = None):
        """
        Inicializa o indexador.

        Args:
            db_path: Caminho para persistir o banco.
                    Default: data/vector_db/
        """
        # Define caminho do banco
        if db_path is None:
            project_root = Path(__file__).resolve().parent.parent.parent
            db_path = project_root / "data" / "vector_db"

        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)

        print(f"üìÅ Banco vetorial em: {self.db_path}")

        # Inicializa ChromaDB com persist√™ncia em disco
        # PersistentClient = dados salvos em disco, sobrevivem entre execu√ß√µes
        self.client = chromadb.PersistentClient(
            path=str(self.db_path),
            settings=Settings(anonymized_telemetry=False),  # Desativa telemetria
        )

        # Inicializa gerenciador de embeddings
        self.embedding_manager = EmbeddingManager()

        # Cria ou obt√©m a collection
        # get_or_create = cria se n√£o existe, obt√©m se j√° existe
        self.collection = self.client.get_or_create_collection(
            name=self.COLLECTION_NAME,
            metadata={"description": "Documentos de contexto do projeto scrapper"},
        )

        print(
            f"üìö Collection '{self.COLLECTION_NAME}' pronta. "
            f"Documentos atuais: {self.collection.count()}"
        )

    def index_file(self, file_path: Path) -> int:
        """
        Indexa um √∫nico arquivo markdown.

        Args:
            file_path: Caminho do arquivo .md

        Returns:
            N√∫mero de chunks indexados
        """
        print(f"  üìÑ Indexando: {file_path.name}")

        # L√™ conte√∫do do arquivo
        content = file_path.read_text(encoding="utf-8")

        # Extrai t√≠tulo
        title = extract_title(content)

        # Divide em chunks
        chunks = chunk_text(content)
        print(f"     ‚Üí {len(chunks)} chunks")

        # Prepara dados para inser√ß√£o
        ids = []
        documents = []
        metadatas = []

        for i, chunk in enumerate(chunks):
            chunk_id = generate_chunk_id(str(file_path), i)

            ids.append(chunk_id)
            documents.append(chunk)
            metadatas.append(
                {
                    "source": str(file_path),
                    "filename": file_path.name,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "title": title,
                }
            )

        # Gera embeddings para todos os chunks de uma vez (mais eficiente)
        embeddings = self.embedding_manager.embed(documents).tolist()

        # Insere no ChromaDB (upsert = insert or update)
        self.collection.upsert(
            ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas
        )

        return len(chunks)

    def index_directory(self, docs_path: Optional[Path] = None) -> Dict[str, int]:
        """
        Indexa todos os arquivos .md de um diret√≥rio.

        Args:
            docs_path: Caminho do diret√≥rio. Default: docs/context/

        Returns:
            Dict com estat√≠sticas {arquivo: n_chunks}
        """
        if docs_path is None:
            project_root = Path(__file__).resolve().parent.parent.parent
            docs_path = project_root / "docs" / "context"

        print(f"\nüîç Indexando documentos de: {docs_path}\n")

        stats = {}
        total_chunks = 0

        # Lista todos os .md no diret√≥rio
        md_files = sorted(docs_path.glob("*.md"))

        for file_path in md_files:
            n_chunks = self.index_file(file_path)
            stats[file_path.name] = n_chunks
            total_chunks += n_chunks

        print(f"\n‚úÖ Indexa√ß√£o completa!")
        print(f"   üìÅ Arquivos: {len(stats)}")
        print(f"   üìÑ Chunks totais: {total_chunks}")
        print(f"   üíæ Banco em: {self.db_path}")

        return stats

    def clear(self) -> None:
        """
        Remove todos os documentos da collection.

        √ötil para re-indexar do zero.
        """
        print("üóëÔ∏è Limpando collection...")
        self.client.delete_collection(self.COLLECTION_NAME)
        self.collection = self.client.create_collection(
            name=self.COLLECTION_NAME,
            metadata={"description": "Documentos de contexto do projeto scrapper"},
        )
        print("‚úÖ Collection limpa!")


# Script execut√°vel diretamente
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Indexa documentos de contexto no ChromaDB"
    )
    parser.add_argument(
        "--clear", action="store_true", help="Limpa o banco antes de indexar"
    )
    parser.add_argument(
        "--path", type=str, help="Caminho alternativo para os documentos"
    )

    args = parser.parse_args()

    indexer = ContextIndexer()

    if args.clear:
        indexer.clear()

    docs_path = Path(args.path) if args.path else None
    indexer.index_directory(docs_path)
